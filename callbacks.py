import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class PerformanceMonitoringCallback(BaseCallbackHandler):
    """LLM í˜¸ì¶œ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬"""
    
    def __init__(self):
        self.start_time: Optional[float] = None
        self.token_usage: Dict[str, Any] = {}
        self.call_count: int = 0
        
    def on_llm_start(
        self, 
        serialized: Dict[str, Any], 
        prompts: List[str], 
        **kwargs: Any
    ) -> None:
        """LLM í˜¸ì¶œì´ ì‹œì‘ë  ë•Œ í˜¸ì¶œ"""
        self.start_time = time.time()
        self.call_count += 1
        print(f"ğŸš€ LLM í˜¸ì¶œ #{self.call_count} ì‹œì‘ - {datetime.now().strftime('%H:%M:%S')}")
        
        # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ì˜ ê¸¸ì´ í™•ì¸
        if prompts:
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompts[0])} ë¬¸ì")
        
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM í˜¸ì¶œì´ ì™„ë£Œë  ë•Œ í˜¸ì¶œ"""
        if self.start_time:
            duration = time.time() - self.start_time
            print(f"âœ… LLM í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ")
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
            if response.generations:
                generation = response.generations[0][0]
                
                # usage_metadataë¥¼ ìš°ì„  í™•ì¸ 
                if hasattr(generation, 'usage_metadata') and generation.usage_metadata:
                    usage = generation.usage_metadata
                    print(f"ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {usage}")
                    self.token_usage = usage
                    
                # êµ¬ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ llm_output í™•ì¸
                elif hasattr(response, 'llm_output') and response.llm_output:
                    usage = response.llm_output.get('token_usage', {})
                    if usage:
                        print(f"ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {usage}")
                        self.token_usage = usage
                        
                # ì‘ë‹µ ê¸¸ì´ ì²´í¬
                if hasattr(generation, 'text'):
                    response_text = generation.text
                    print(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì")
        
    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:
        """LLM í˜¸ì¶œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ë•Œ í˜¸ì¶œ"""
        print(f"âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜: {str(error)}")
        
    def get_statistics(self) -> Dict[str, Any]:
        """í˜„ì¬ê¹Œì§€ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜"""
        return {
            "total_calls": self.call_count,
            "last_token_usage": self.token_usage
        }
    
